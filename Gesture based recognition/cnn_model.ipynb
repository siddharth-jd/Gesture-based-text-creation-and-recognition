{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VT_rjUEUMQ_s",
        "outputId": "5bb3ec7f-8d28-4459-d81c-bbd2d84189c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import datasets, regularizers\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels)  = datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Use part of the training data for validation\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow(train_images, train_labels, batch_size=32, subset='training')\n",
        "val_generator = train_datagen.flow(train_images, train_labels, batch_size=32, subset='validation')\n",
        "test_generator = test_datagen.flow(test_images, test_labels, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # 10 classes for MNIST\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bPkWtIcwMcMU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qnGT1bxMMni0",
        "outputId": "8fad1755-fb38-4c5c-b209-cef33934ec76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 28, 28, 64)        256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 28, 28, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 14, 14, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 14, 14, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 7, 7, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 7, 7, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 3, 3, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 3, 3, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1180160   \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2335178 (8.91 MB)\n",
            "Trainable params: 2332362 (8.90 MB)\n",
            "Non-trainable params: 2816 (11.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data=val_generator,\n",
        "                    epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZqKrqysENFtB",
        "outputId": "acf03957-9d94-4469-a740-bad0db460dd7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 38s 18ms/step - loss: 1.0871 - accuracy: 0.8681 - val_loss: 1.1377 - val_accuracy: 0.7937\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.4899 - accuracy: 0.9470 - val_loss: 0.3911 - val_accuracy: 0.9630\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.4057 - accuracy: 0.9570 - val_loss: 0.3587 - val_accuracy: 0.9721\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.3965 - accuracy: 0.9593 - val_loss: 2.0087 - val_accuracy: 0.5462\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.3638 - accuracy: 0.9643 - val_loss: 0.3374 - val_accuracy: 0.9728\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.3564 - accuracy: 0.9662 - val_loss: 1.2030 - val_accuracy: 0.6842\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.3376 - accuracy: 0.9685 - val_loss: 0.3280 - val_accuracy: 0.9693\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.3202 - accuracy: 0.9715 - val_loss: 0.3019 - val_accuracy: 0.9775\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.3081 - accuracy: 0.9720 - val_loss: 0.2664 - val_accuracy: 0.9828\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.3056 - accuracy: 0.9733 - val_loss: 0.2696 - val_accuracy: 0.9827\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.2874 - accuracy: 0.9748 - val_loss: 0.2454 - val_accuracy: 0.9868\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.2814 - accuracy: 0.9754 - val_loss: 0.2483 - val_accuracy: 0.9768\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.2616 - accuracy: 0.9769 - val_loss: 0.2580 - val_accuracy: 0.9829\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.2615 - accuracy: 0.9776 - val_loss: 0.2333 - val_accuracy: 0.9858\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 0.2511 - accuracy: 0.9788 - val_loss: 0.2485 - val_accuracy: 0.9818\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.2430 - accuracy: 0.9789 - val_loss: 0.2325 - val_accuracy: 0.9857\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.2351 - accuracy: 0.9800 - val_loss: 0.3290 - val_accuracy: 0.9575\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.2323 - accuracy: 0.9809 - val_loss: 1.0668 - val_accuracy: 0.7153\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 26s 18ms/step - loss: 0.2144 - accuracy: 0.9825 - val_loss: 3.3767 - val_accuracy: 0.2652\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 26s 17ms/step - loss: 0.2127 - accuracy: 0.9817 - val_loss: 0.4862 - val_accuracy: 0.8912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0lMnHGspPmcU",
        "outputId": "d3df0770-87b4-488c-a844-29dab979a2ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.3983 - accuracy: 0.9182\n",
            "Test accuracy: 0.9182000160217285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('firstModel.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fN01zgeYPzB1",
        "outputId": "fc4aae93-0d6d-43be-ebb4-3ea80bddd913"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}